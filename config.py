from util.time_generation import TimeGeneration
from util.utils import get_random_data_arrive
from random_arrival_data import ran_num, ran_round
import os

SERVER_ADDR= 'localhost'   # When running in a real distributed setting, change to the server's IP address
SERVER_PORT = 51000

dataset_file_path = os.path.join(os.path.dirname(__file__), 'datasets')
results_file_path = os.path.join(os.path.dirname(__file__), 'results')
# single_run_results_file_path = results_file_path + '/SingleRun.csv'
multi_run_results_file_path = results_file_path + '/MultipleRuns.csv'

# Model, dataset, and control parameter configurations for MNIST with SVM
# dataset = 'MNIST_ORIG_EVEN_ODD'  # Use for SVM model
# model_name = 'ModelSVMSmooth'
# control_param_phi = 0.025   # Good for MNIST with smooth SVM

# Model, dataset, and control parameter configurations for MNIST with CNN
# dataset = 'MNIST_ORIG_ALL_LABELS'  # Use for CNN model
# model_name = 'ModelCNNMnist'
# control_param_phi = 0.00005   # Good for CNN

# Model, dataset, and control parameter configurations for CIFAR-10 with CNN
dataset = 'CIFAR_10'
model_name = 'ModelCNNCifar10'
control_param_phi = 0.00005   # Good for CNN

n_nodes = 3  # Specifies the total number of clients

moving_average_holding_param = 0.0  # Moving average coefficient to smooth the estimation of beta, delta, and rho

step_size = 0.005
total_k = 3000 # Communication Rounds
ts = 120  # tau * s
resource_budget = total_k + ts
time_budget = 60000

# Setting batch_size equal to total_data makes the system use deterministic gradient descent;
# Setting batch_size equal < total_data makes the system use stochastic gradient descent.
# batch_size = 1000  # Value for deterministic gradient descent
# total_data = 1000  # Value for deterministic gradient descent
batch_size = 60  # 100  # Value for stochastic gradient descent, (Initial batch-size)
total_data = 60000  # 60000  #Value for stochastic gradient descent

# Choose whether to run a single instance and plot the instantaneous results or
# run multiple instances and plot average results
single_run = True

# Choose whether to estimate beta and delta in all runs, including those where tau is not adaptive,
# this is useful for getting statistics. NOTE: Enabling this may change the communication time when using
# real-world measurements for resource consumption
estimate_beta_delta_in_all_runs = False

# If true, the weight corresponding to minimum loss (the loss is estimated if using stochastic gradient descent) is
# returned. If false, the weight at the end is returned. Setting use_min_loss = True corresponds to the latest
# theoretical bound for the **DISTRIBUTED** case.
# For the **CENTRALIZED** case, set use_min_loss = False,
# because convergence of the final value can be guaranteed in the centralized case.
use_min_loss = False

evaluate_train_loss_per_round = True
evaluate_acc_per_round = True
evaluate_gap = 10

batch_train_loss = True # If true, Use batch loss to represent whole train data set loss to save time
# Specifies the number of iterations the client uses the same minibatch, using the same minibatch can reduce
# the processing time at the client, but may cause a worse model accuracy.
# We use the same minibatch only when the client receives tau_config = 1
num_iterations_with_same_minibatch_for_tau_equals_one = 1

# Specifies whether all the data should be read when using stochastic gradient descent.
# Reading all the data requires much more memory but should avoid slowing down due to file reading.
read_all_data_for_stochastic = False

MAX_CASE = 7  # Specifies the maximum number of cases, this should be a constant equal to 4
tau_max = 100  # Specifies the maximum value of tau

# tau_setup = -1 is for the proposed adaptive control algorithm, other values of tau correspond to fixed tau values
if not single_run:
    # tau_setup_all = [-1, 1, 2, 3, 5, 7, 10, 20, 30, 50, 70, 100]
    # sim_runs = range(0, 2)  #Specifies the simulation seeds in each simulation round
    # case_range = range(0, MAX_CASE)
    tau_setup_all = [2,5,10]
    sim_runs = [0]  # Specifies the simulation seeds in each simulation round
    case_range = [1]
else:
    case_range = [6]   # Change if we want single run with other case, should only have one case
    tau_setup_all = [-1]   # Should only have one value, -1 is the adaptive algorithm
    sim_runs = [0]   # Should only have one value, the value specifies the random seed

bs_dis = True  # Choose whether to use proposed heterogeneous batch-size
no_strag = False # If true, distribute the batch size according to no-straggler principle

status = 'U'

if bs_dis is True:
    status = 'A'
elif no_strag is True:
    status = 'N'

max_time = 2000  # Total time budget in seconds

# If time_gen is None, use actual measured time. Else, use time generated by the TimeGeneration class.
time_gen = None
# time_gen = TimeGeneration(1, 0.0, 1e-10, 0.0, 0.0, 0.0)

multiply_global = 1.0
multiply_local = 1.0

# These numbers are from measurement on stochastic gradient descent on SVM smooth with MNIST even/odd data.
# time_gen = TimeGeneration(multiply_local * 0.013015156, multiply_local * 0.006946299, 1e-10,
#                           multiply_global * 0.131604348, multiply_global * 0.053873234, 1e-10)

# These numbers are from measurement on CENTRALIZED stochastic gradient descent on SVM smooth with MNIST even/odd data.
# time_gen = TimeGeneration(multiply_local * 0.009974248, multiply_local * 0.011922926, 1e-10,
#                           0.0, 0.0, 0.0)

# These numbers are from measurement on deterministic gradient descent on SVM smooth with MNIST even/odd data.
# time_gen = []
# time_gen.append(TimeGeneration(multiply_local * 0.020613052, multiply_local * 0.008154439, 1e-10,
#                                multiply_global * 0.137093837, multiply_global * 0.05548447, 1e-10))  # Case 1
# time_gen.append(TimeGeneration(multiply_local * 0.021810727, multiply_local * 0.008042984, 1e-10,
#                                multiply_global * 0.12322071, multiply_global * 0.048079171, 1e-10))  # Case 2
# time_gen.append(TimeGeneration(multiply_local * 0.095353094, multiply_local * 0.016688657, 1e-10,
#                                multiply_global * 0.157255906, multiply_global * 0.066722225, 1e-10))  # Case 3
# time_gen.append(TimeGeneration(multiply_local * 0.022075891, multiply_local * 0.008528005, 1e-10,
#                                multiply_global * 0.108598094, multiply_global * 0.044627335, 1e-10))  # Case 4

stream_mode = False  # Whether use streaming data or static data
use_preset_param = False  # Whether use preset param below or the online estimated param
preset_param = [40.0, 1.6, 30.0] # beta, rho, c
# preset_delta = 4.912

# Data Arrival Pattern
# Smooth arrival case[2] in default, ablation use [6]
# arrive_round = [[(j+1)*100 for j in range(15)] for i in range(n_nodes)] # Data arrival pattern 1. arrive round 2. the number of the arrived data
# arrive_num = [[3000 for j in range(15)] for i in range(n_nodes)]

# buff_size = [10000 for i in range(n_nodes)] # Define the buffer size for each client
# init_buff = [5000 for i in range(n_nodes)] # initialize the number of the data sample in the buffer

# Smooth arrival case Faster [2] / [6]
# arrive_round = [[(j+1)*150 for j in range(5)] for i in range(n_nodes)] # Data arrival pattern 1. arrive round 2. the number of the arrived data
# arrive_num = [[8000 for j in range(5)] for i in range(n_nodes)]

# buff_size = [10000 for i in range(n_nodes)] # Define the buffer size for each client
# init_buff = [10000 for i in range(n_nodes)] # initialize the number of the data sample in the buffer

# Burst arrival case [6]
# arrive_round = [[1000] for i in range(n_nodes)] # Data arrival pattern 1. arrive round 2. the number of the arrived data
# arrive_num = [[25000 for j in range(1)] for i in range(n_nodes)]
#
# buff_size = [25000 for i in range(n_nodes)] # Define the buffer size for each client
# init_buff = [25000 for i in range(n_nodes)] # initialize the number of the data sample in the buffer

# Random arrival case [6] Auto-generate
# buff_size = [10000 for i in range(n_nodes)] # Define the buffer size for each client
# init_buff = [10000 for i in range(n_nodes)] # initialize the number of the data sample in the buffer
# [arrive_round, arrive_num] = get_random_data_arrive(n_nodes, 5, total_k, 40000)

# Random arrival case [6] Manual-generate for comparison
buff_size = [10000 for i in range(n_nodes)] # Define the buffer size for each client
init_buff = [10000 for i in range(n_nodes)] # initialize the number of the data sample in the buffer
[arrive_round, arrive_num] = [ran_round, ran_num]

# Sampling methods
samp_method = 'ran' # res, ran, fifo

# Adaptive alg : True-JSAC or False-ours/uniform
use_dynamic_tau = False

if use_dynamic_tau is True:
    status = 'J'

# Adaptive Learning rate
adaptive_lr = False

single_run_results_file_path = results_file_path + '/SingleRun_' + status + '_' + str(tau_setup_all[0]) + '_' + str(batch_size) + '_' + samp_method + '.csv'
